{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1fTSeOleUnR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-17 13:42:50.497050: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-17 13:42:50.827590: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-17 13:42:51.856402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-17 13:42:54.892486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Packages for training the model and working with the dataset.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2DS6VeSfz_h"
      },
      "source": [
        "# Merging and Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GFN6FivvemVq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(silent=False):\n",
        "    # List of dataset files we want to merge.\n",
        "    dataset_file_names = [\n",
        "        'recipes_raw_nosource_ar.json',\n",
        "        'recipes_raw_nosource_epi.json',\n",
        "        'recipes_raw_nosource_fn.json',\n",
        "    ]\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for dataset_file_name in dataset_file_names:\n",
        "        dataset_file_path = f'./sample_data/{dataset_file_name}'\n",
        "\n",
        "        with open(dataset_file_path) as dataset_file:\n",
        "            json_data_dict = json.load(dataset_file)\n",
        "            json_data_list = list(json_data_dict.values())\n",
        "            dict_keys = [key for key in json_data_list[0]]\n",
        "            dict_keys.sort()\n",
        "            dataset += json_data_list\n",
        "\n",
        "            # This code block outputs the summary for each dataset.\n",
        "            if silent == False:\n",
        "                print(dataset_file_path)\n",
        "                print('===========================================')\n",
        "                print('Number of examples: ', len(json_data_list), '\\n')\n",
        "                print('Example object keys:\\n', dict_keys, '\\n')\n",
        "                print('Example object:\\n', json_data_list[0], '\\n')\n",
        "                print('Required keys:\\n')\n",
        "                print('  title: ', json_data_list[0]['title'], '\\n')\n",
        "                print('  ingredients: ', json_data_list[0]['ingredients'], '\\n')\n",
        "                print('  instructions: ', json_data_list[0]['instructions'])\n",
        "                print('\\n\\n')\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset_raw = load_dataset(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChWWG2FLgekS"
      },
      "source": [
        "# Filter out recipe with missing feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GR7CbB4IexsX"
      },
      "outputs": [],
      "source": [
        "def recipe_validate_required_fields(recipe):\n",
        "    required_keys = ['title', 'ingredients', 'instructions']\n",
        "\n",
        "    if not recipe:\n",
        "        return False\n",
        "\n",
        "    for required_key in required_keys:\n",
        "        if not recipe[required_key]:\n",
        "            return False\n",
        "\n",
        "        if type(recipe[required_key]) == list and len(recipe[required_key]) == 0:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "dataset_validated = [recipe for recipe in dataset_raw if recipe_validate_required_fields(recipe)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKQVeTNBUNug"
      },
      "source": [
        "# Stringify recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8D_Ocx4VgnxC"
      },
      "outputs": [],
      "source": [
        "STOP_WORD_TITLE = '???? '\n",
        "STOP_WORD_INGREDIENTS = '\\n????\\n\\n'\n",
        "STOP_WORD_INSTRUCTIONS = '\\n????\\n\\n'\n",
        "\n",
        "def recipe_to_string(recipe):\n",
        "    # This string is presented as a part of recipes so we need to clean it up.\n",
        "    noize_string = 'ADVERTISEMENT'\n",
        "\n",
        "    title = recipe['title']\n",
        "    ingredients = recipe['ingredients']\n",
        "    instructions = recipe['instructions'].split('\\n')\n",
        "\n",
        "    ingredients_string = ''\n",
        "    for ingredient in ingredients:\n",
        "        ingredient = ingredient.replace(noize_string, '')\n",
        "        if ingredient:\n",
        "            ingredients_string += f'• {ingredient}\\n'\n",
        "\n",
        "    instructions_string = ''\n",
        "    for instruction in instructions:\n",
        "        instruction = instruction.replace(noize_string, '')\n",
        "        if instruction:\n",
        "            instructions_string += f'▪︎ {instruction}\\n'\n",
        "\n",
        "    return f'{STOP_WORD_TITLE}{title}\\n{STOP_WORD_INGREDIENTS}{ingredients_string}{STOP_WORD_INSTRUCTIONS}{instructions_string}'\n",
        "\n",
        "dataset_stringified = [recipe_to_string(recipe) for recipe in dataset_validated]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4pbryUVrpg"
      },
      "source": [
        "# Filter out long recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q6gveOvIVPMZ"
      },
      "outputs": [],
      "source": [
        "MAX_RECIPE_LENGTH = 2000\n",
        "def filter_recipes_by_length(recipe_test):\n",
        "    return len(recipe_test) <= MAX_RECIPE_LENGTH\n",
        "\n",
        "dataset_filtered = [recipe_text for recipe_text in dataset_stringified if filter_recipes_by_length(recipe_text)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qfJJkEsWW9x"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QbKylCFVyRl",
        "outputId": "97f0b222-39a4-4581-983d-036e12449a7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '',\n",
              " 'lower': False,\n",
              " 'split': '',\n",
              " 'char_level': True,\n",
              " 'oov_token': None,\n",
              " 'document_count': 100002,\n",
              " 'word_counts': '{\"\\\\u2423\": 1, \"?\": 1200337, \" \": 17456341, \"S\": 269334, \"l\": 3799351, \"o\": 5962595, \"w\": 960376, \"C\": 222127, \"k\": 887422, \"e\": 9257735, \"r\": 4741353, \"h\": 2908968, \"i\": 4891153, \"c\": 2872419, \"n\": 5282692, \"a\": 6042166, \"d\": 3086749, \"D\": 63726, \"u\": 2706138, \"m\": 1786991, \"p\": 2668709, \"g\": 1691743, \"s\": 4684699, \"\\\\n\": 1949725, \"\\\\u2022\": 919860, \"4\": 231801, \",\": 1125887, \"b\": 1388955, \"t\": 5971681, \"v\": 743650, \"2\": 492133, \"(\": 144468, \"1\": 851115, \"0\": 144643, \".\": 1048251, \"7\": 31019, \"5\": 153573, \")\": 144461, \"f\": 1038022, \"y\": 663513, \"\\\\u25aa\": 329932, \"\\\\ufe0e\": 329932, \"P\": 199874, \"6\": 51190, \"H\": 43774, \"A\": 133626, \"3\": 212769, \"R\": 100857, \"x\": 200639, \"/\": 344177, \"I\": 81359, \"L\": 45967, \"8\": 55146, \"9\": 17642, \"B\": 123389, \"M\": 78391, \"F\": 103981, \"j\": 109527, \"-\": 218143, \"W\": 61318, \"\\\\u00ae\": 10151, \"N\": 12762, \"q\": 69291, \"T\": 100919, \";\": 71813, \"\\'\": 26737, \"Z\": 2420, \"z\": 115415, \"G\": 51865, \":\": 31090, \"E\": 18534, \"K\": 18337, \"X\": 382, \"\\\\\"\": 6409, \"O\": 28831, \"Y\": 6031, \"\\\\u2122\": 538, \"Q\": 3894, \"J\": 10231, \"!\": 3002, \"U\": 14060, \"V\": 12137, \"&\": 1038, \"+\": 87, \"=\": 113, \"%\": 993, \"*\": 3216, \"\\\\u00a9\": 99, \"[\": 30, \"]\": 31, \"\\\\u00e9\": 6663, \"<\": 76, \">\": 86, \"\\\\u00bd\": 166, \"#\": 168, \"\\\\u00f1\": 884, \"\\\\u2019\": 111, \"\\\\u00b0\": 6750, \"\\\\u201d\": 6, \"$\": 84, \"@\": 5, \"{\": 8, \"}\": 9, \"\\\\u2013\": 1224, \"\\\\u0096\": 7, \"\\\\u00e0\": 24, \"\\\\u00e2\": 106, \"\\\\u00e8\": 830, \"\\\\u00e1\": 70, \"\\\\u2014\": 215, \"\\\\u2044\": 16, \"\\\\u00ee\": 404, \"\\\\u00e7\": 171, \"_\": 26, \"\\\\u00fa\": 48, \"\\\\u00ef\": 43, \"\\\\u201a\": 20, \"\\\\u00fb\": 36, \"\\\\u00f3\": 74, \"\\\\u00ed\": 126, \"\\\\u25ca\": 4, \"\\\\u00f9\": 12, \"\\\\u00d7\": 6, \"\\\\u00ec\": 8, \"\\\\u00fc\": 29, \"\\\\u2031\": 4, \"\\\\u00ba\": 19, \"\\\\u201c\": 4, \"\\\\u00ad\": 25, \"\\\\u00ea\": 27, \"\\\\u00f6\": 9, \"\\\\u0301\": 11, \"\\\\u00f4\": 8, \"\\\\u00c1\": 1, \"\\\\u00be\": 23, \"\\\\u00bc\": 95, \"\\\\u00eb\": 2, \"\\\\u0097\": 2, \"\\\\u215b\": 3, \"\\\\u2027\": 4, \"\\\\u00e4\": 15, \"\\\\u001a\": 2, \"\\\\u00f8\": 2, \"\\\\ufffd\": 20, \"\\\\u02da\": 6, \"\\\\u00bf\": 264, \"\\\\u2153\": 2, \"|\": 2, \"\\\\u00e5\": 3, \"\\\\u00a4\": 1, \"\\\\u201f\": 1, \"\\\\u00a7\": 5, \"\\\\ufb02\": 3, \"\\\\u00a0\": 1, \"\\\\u01b0\": 2, \"\\\\u01a1\": 1, \"\\\\u0103\": 1, \"\\\\u0300\": 1, \"\\\\u00bb\": 6, \"`\": 3, \"\\\\u0092\": 2, \"\\\\u215e\": 1, \"\\\\u202d\": 4, \"\\\\u00b4\": 2, \"\\\\u2012\": 2, \"\\\\u00c9\": 40, \"\\\\u00da\": 14, \"\\\\u20ac\": 1, \"\\\\\\\\\": 5, \"~\": 1, \"\\\\u0095\": 1, \"\\\\u00c2\": 2}',\n",
              " 'word_docs': '{\"\\\\u2423\": 1, \"f\": 98120, \"D\": 40732, \"s\": 99993, \" \": 100001, \"A\": 60713, \"S\": 89048, \"c\": 99979, \"r\": 99996, \"5\": 65320, \"7\": 24318, \"P\": 79174, \"H\": 31809, \"e\": 100001, \")\": 67439, \"1\": 99109, \"t\": 99991, \"\\\\u25aa\": 100001, \"h\": 99950, \"n\": 99987, \"g\": 99764, \".\": 99952, \"0\": 61787, \"6\": 35100, \"3\": 78938, \"4\": 82252, \"w\": 99016, \"o\": 99994, \"m\": 99777, \"\\\\n\": 100001, \"y\": 96176, \"l\": 99995, \"\\\\ufe0e\": 100001, \",\": 98313, \"k\": 97105, \"2\": 96533, \"a\": 99994, \"b\": 99491, \"\\\\u2022\": 100001, \"p\": 99835, \"(\": 67452, \"d\": 99983, \"i\": 99996, \"v\": 97637, \"u\": 99950, \"C\": 83131, \"?\": 100001, \"R\": 53876, \"9\": 14079, \"I\": 46567, \"x\": 71957, \"/\": 88846, \"L\": 31995, \"8\": 36888, \"M\": 48188, \"B\": 64114, \"j\": 47295, \"F\": 57778, \"-\": 74511, \"W\": 38849, \"q\": 36415, \"\\\\u00ae\": 5814, \"N\": 9944, \"T\": 53584, \";\": 33783, \"z\": 42302, \"G\": 35266, \"Z\": 2176, \"\\'\": 18069, \":\": 18108, \"E\": 12128, \"K\": 14771, \"X\": 320, \"\\\\\"\": 2603, \"O\": 20024, \"Y\": 5124, \"\\\\u2122\": 448, \"Q\": 3136, \"J\": 8197, \"!\": 2422, \"U\": 10575, \"V\": 9683, \"&\": 748, \"=\": 48, \"+\": 32, \"%\": 717, \"*\": 1773, \"\\\\u00a9\": 91, \"[\": 25, \"]\": 26, \"\\\\u00e9\": 2441, \">\": 33, \"<\": 27, \"\\\\u00bd\": 81, \"#\": 139, \"\\\\u00f1\": 420, \"\\\\u2019\": 64, \"\\\\u00b0\": 3037, \"\\\\u201d\": 3, \"$\": 49, \"@\": 4, \"}\": 8, \"{\": 7, \"\\\\u2013\": 489, \"\\\\u0096\": 7, \"\\\\u00e0\": 21, \"\\\\u00e2\": 45, \"\\\\u00e8\": 331, \"\\\\u00e1\": 36, \"\\\\u2014\": 95, \"\\\\u2044\": 9, \"\\\\u00ee\": 120, \"\\\\u00e7\": 120, \"_\": 8, \"\\\\u00fa\": 25, \"\\\\u00ef\": 24, \"\\\\u201a\": 10, \"\\\\u00fb\": 29, \"\\\\u00f3\": 40, \"\\\\u00ed\": 51, \"\\\\u25ca\": 2, \"\\\\u00f9\": 6, \"\\\\u00d7\": 4, \"\\\\u00ec\": 4, \"\\\\u00fc\": 19, \"\\\\u2031\": 2, \"\\\\u00ba\": 9, \"\\\\u201c\": 2, \"\\\\u00ad\": 11, \"\\\\u00ea\": 4, \"\\\\u00f6\": 4, \"\\\\u0301\": 6, \"\\\\u00f4\": 5, \"\\\\u00c1\": 1, \"\\\\u00bc\": 55, \"\\\\u00be\": 18, \"\\\\u00eb\": 2, \"\\\\u0097\": 1, \"\\\\u215b\": 2, \"\\\\u2027\": 3, \"\\\\u00e4\": 8, \"\\\\u001a\": 1, \"\\\\u00f8\": 1, \"\\\\ufffd\": 4, \"\\\\u02da\": 3, \"\\\\u00bf\": 191, \"\\\\u2153\": 1, \"|\": 2, \"\\\\u00e5\": 1, \"\\\\u00a4\": 1, \"\\\\u00a7\": 3, \"\\\\u201f\": 1, \"\\\\ufb02\": 1, \"\\\\u0103\": 1, \"\\\\u00a0\": 1, \"\\\\u01a1\": 1, \"\\\\u01b0\": 1, \"\\\\u0300\": 1, \"\\\\u00bb\": 2, \"`\": 3, \"\\\\u0092\": 2, \"\\\\u215e\": 1, \"\\\\u202d\": 1, \"\\\\u00b4\": 1, \"\\\\u2012\": 1, \"\\\\u00c9\": 15, \"\\\\u00da\": 5, \"\\\\u20ac\": 1, \"\\\\\\\\\": 5, \"~\": 1, \"\\\\u0095\": 1, \"\\\\u00c2\": 1}',\n",
              " 'index_docs': '{\"1\": 100001, \"161\": 1, \"23\": 98120, \"56\": 40732, \"9\": 99993, \"45\": 60713, \"34\": 89048, \"13\": 99979, \"8\": 99996, \"41\": 65320, \"64\": 24318, \"40\": 79174, \"62\": 31809, \"2\": 100001, \"44\": 67439, \"27\": 99109, \"4\": 99991, \"32\": 100001, \"12\": 99950, \"6\": 99987, \"18\": 99764, \"22\": 99952, \"42\": 61787, \"60\": 35100, \"38\": 78938, \"35\": 82252, \"24\": 99016, \"5\": 99994, \"17\": 99777, \"16\": 100001, \"29\": 96176, \"10\": 99995, \"33\": 100001, \"21\": 98313, \"26\": 97105, \"30\": 96533, \"3\": 99994, \"19\": 99491, \"25\": 100001, \"15\": 99835, \"43\": 67452, \"11\": 99983, \"7\": 99996, \"28\": 97637, \"14\": 99950, \"36\": 83131, \"20\": 100001, \"51\": 53876, \"69\": 14079, \"52\": 46567, \"39\": 71957, \"31\": 88846, \"61\": 31995, \"58\": 36888, \"53\": 48188, \"46\": 64114, \"48\": 47295, \"49\": 57778, \"37\": 74511, \"57\": 38849, \"55\": 36415, \"74\": 5814, \"71\": 9944, \"50\": 53584, \"54\": 33783, \"47\": 42302, \"59\": 35266, \"82\": 2176, \"66\": 18069, \"63\": 18108, \"67\": 12128, \"68\": 14771, \"90\": 320, \"77\": 2603, \"65\": 20024, \"78\": 5124, \"88\": 448, \"79\": 3136, \"73\": 8197, \"81\": 2422, \"70\": 10575, \"72\": 9683, \"84\": 748, \"97\": 48, \"102\": 32, \"85\": 717, \"80\": 1773, \"100\": 91, \"113\": 25, \"112\": 26, \"76\": 2441, \"103\": 33, \"105\": 27, \"95\": 81, \"94\": 139, \"86\": 420, \"98\": 64, \"75\": 3037, \"134\": 3, \"104\": 49, \"138\": 4, \"128\": 8, \"130\": 7, \"83\": 489, \"133\": 7, \"118\": 21, \"99\": 45, \"87\": 331, \"107\": 36, \"92\": 95, \"123\": 9, \"89\": 120, \"93\": 120, \"116\": 8, \"108\": 25, \"109\": 24, \"120\": 10, \"111\": 29, \"106\": 40, \"96\": 51, \"141\": 2, \"126\": 6, \"135\": 4, \"131\": 4, \"114\": 19, \"142\": 2, \"122\": 9, \"143\": 2, \"117\": 11, \"115\": 4, \"129\": 4, \"127\": 6, \"132\": 5, \"162\": 1, \"101\": 55, \"119\": 18, \"150\": 2, \"151\": 1, \"146\": 2, \"144\": 3, \"124\": 8, \"152\": 1, \"153\": 1, \"121\": 4, \"136\": 3, \"91\": 191, \"154\": 1, \"155\": 2, \"147\": 1, \"163\": 1, \"139\": 3, \"164\": 1, \"148\": 1, \"167\": 1, \"165\": 1, \"166\": 1, \"156\": 1, \"168\": 1, \"137\": 2, \"149\": 3, \"157\": 2, \"169\": 1, \"145\": 1, \"158\": 1, \"159\": 1, \"110\": 15, \"125\": 5, \"170\": 1, \"140\": 5, \"171\": 1, \"172\": 1, \"160\": 1}',\n",
              " 'index_word': '{\"1\": \" \", \"2\": \"e\", \"3\": \"a\", \"4\": \"t\", \"5\": \"o\", \"6\": \"n\", \"7\": \"i\", \"8\": \"r\", \"9\": \"s\", \"10\": \"l\", \"11\": \"d\", \"12\": \"h\", \"13\": \"c\", \"14\": \"u\", \"15\": \"p\", \"16\": \"\\\\n\", \"17\": \"m\", \"18\": \"g\", \"19\": \"b\", \"20\": \"?\", \"21\": \",\", \"22\": \".\", \"23\": \"f\", \"24\": \"w\", \"25\": \"\\\\u2022\", \"26\": \"k\", \"27\": \"1\", \"28\": \"v\", \"29\": \"y\", \"30\": \"2\", \"31\": \"/\", \"32\": \"\\\\u25aa\", \"33\": \"\\\\ufe0e\", \"34\": \"S\", \"35\": \"4\", \"36\": \"C\", \"37\": \"-\", \"38\": \"3\", \"39\": \"x\", \"40\": \"P\", \"41\": \"5\", \"42\": \"0\", \"43\": \"(\", \"44\": \")\", \"45\": \"A\", \"46\": \"B\", \"47\": \"z\", \"48\": \"j\", \"49\": \"F\", \"50\": \"T\", \"51\": \"R\", \"52\": \"I\", \"53\": \"M\", \"54\": \";\", \"55\": \"q\", \"56\": \"D\", \"57\": \"W\", \"58\": \"8\", \"59\": \"G\", \"60\": \"6\", \"61\": \"L\", \"62\": \"H\", \"63\": \":\", \"64\": \"7\", \"65\": \"O\", \"66\": \"\\'\", \"67\": \"E\", \"68\": \"K\", \"69\": \"9\", \"70\": \"U\", \"71\": \"N\", \"72\": \"V\", \"73\": \"J\", \"74\": \"\\\\u00ae\", \"75\": \"\\\\u00b0\", \"76\": \"\\\\u00e9\", \"77\": \"\\\\\"\", \"78\": \"Y\", \"79\": \"Q\", \"80\": \"*\", \"81\": \"!\", \"82\": \"Z\", \"83\": \"\\\\u2013\", \"84\": \"&\", \"85\": \"%\", \"86\": \"\\\\u00f1\", \"87\": \"\\\\u00e8\", \"88\": \"\\\\u2122\", \"89\": \"\\\\u00ee\", \"90\": \"X\", \"91\": \"\\\\u00bf\", \"92\": \"\\\\u2014\", \"93\": \"\\\\u00e7\", \"94\": \"#\", \"95\": \"\\\\u00bd\", \"96\": \"\\\\u00ed\", \"97\": \"=\", \"98\": \"\\\\u2019\", \"99\": \"\\\\u00e2\", \"100\": \"\\\\u00a9\", \"101\": \"\\\\u00bc\", \"102\": \"+\", \"103\": \">\", \"104\": \"$\", \"105\": \"<\", \"106\": \"\\\\u00f3\", \"107\": \"\\\\u00e1\", \"108\": \"\\\\u00fa\", \"109\": \"\\\\u00ef\", \"110\": \"\\\\u00c9\", \"111\": \"\\\\u00fb\", \"112\": \"]\", \"113\": \"[\", \"114\": \"\\\\u00fc\", \"115\": \"\\\\u00ea\", \"116\": \"_\", \"117\": \"\\\\u00ad\", \"118\": \"\\\\u00e0\", \"119\": \"\\\\u00be\", \"120\": \"\\\\u201a\", \"121\": \"\\\\ufffd\", \"122\": \"\\\\u00ba\", \"123\": \"\\\\u2044\", \"124\": \"\\\\u00e4\", \"125\": \"\\\\u00da\", \"126\": \"\\\\u00f9\", \"127\": \"\\\\u0301\", \"128\": \"}\", \"129\": \"\\\\u00f6\", \"130\": \"{\", \"131\": \"\\\\u00ec\", \"132\": \"\\\\u00f4\", \"133\": \"\\\\u0096\", \"134\": \"\\\\u201d\", \"135\": \"\\\\u00d7\", \"136\": \"\\\\u02da\", \"137\": \"\\\\u00bb\", \"138\": \"@\", \"139\": \"\\\\u00a7\", \"140\": \"\\\\\\\\\", \"141\": \"\\\\u25ca\", \"142\": \"\\\\u2031\", \"143\": \"\\\\u201c\", \"144\": \"\\\\u2027\", \"145\": \"\\\\u202d\", \"146\": \"\\\\u215b\", \"147\": \"\\\\u00e5\", \"148\": \"\\\\ufb02\", \"149\": \"`\", \"150\": \"\\\\u00eb\", \"151\": \"\\\\u0097\", \"152\": \"\\\\u001a\", \"153\": \"\\\\u00f8\", \"154\": \"\\\\u2153\", \"155\": \"|\", \"156\": \"\\\\u01b0\", \"157\": \"\\\\u0092\", \"158\": \"\\\\u00b4\", \"159\": \"\\\\u2012\", \"160\": \"\\\\u00c2\", \"161\": \"\\\\u2423\", \"162\": \"\\\\u00c1\", \"163\": \"\\\\u00a4\", \"164\": \"\\\\u201f\", \"165\": \"\\\\u00a0\", \"166\": \"\\\\u01a1\", \"167\": \"\\\\u0103\", \"168\": \"\\\\u0300\", \"169\": \"\\\\u215e\", \"170\": \"\\\\u20ac\", \"171\": \"~\", \"172\": \"\\\\u0095\"}',\n",
              " 'word_index': '{\" \": 1, \"e\": 2, \"a\": 3, \"t\": 4, \"o\": 5, \"n\": 6, \"i\": 7, \"r\": 8, \"s\": 9, \"l\": 10, \"d\": 11, \"h\": 12, \"c\": 13, \"u\": 14, \"p\": 15, \"\\\\n\": 16, \"m\": 17, \"g\": 18, \"b\": 19, \"?\": 20, \",\": 21, \".\": 22, \"f\": 23, \"w\": 24, \"\\\\u2022\": 25, \"k\": 26, \"1\": 27, \"v\": 28, \"y\": 29, \"2\": 30, \"/\": 31, \"\\\\u25aa\": 32, \"\\\\ufe0e\": 33, \"S\": 34, \"4\": 35, \"C\": 36, \"-\": 37, \"3\": 38, \"x\": 39, \"P\": 40, \"5\": 41, \"0\": 42, \"(\": 43, \")\": 44, \"A\": 45, \"B\": 46, \"z\": 47, \"j\": 48, \"F\": 49, \"T\": 50, \"R\": 51, \"I\": 52, \"M\": 53, \";\": 54, \"q\": 55, \"D\": 56, \"W\": 57, \"8\": 58, \"G\": 59, \"6\": 60, \"L\": 61, \"H\": 62, \":\": 63, \"7\": 64, \"O\": 65, \"\\'\": 66, \"E\": 67, \"K\": 68, \"9\": 69, \"U\": 70, \"N\": 71, \"V\": 72, \"J\": 73, \"\\\\u00ae\": 74, \"\\\\u00b0\": 75, \"\\\\u00e9\": 76, \"\\\\\"\": 77, \"Y\": 78, \"Q\": 79, \"*\": 80, \"!\": 81, \"Z\": 82, \"\\\\u2013\": 83, \"&\": 84, \"%\": 85, \"\\\\u00f1\": 86, \"\\\\u00e8\": 87, \"\\\\u2122\": 88, \"\\\\u00ee\": 89, \"X\": 90, \"\\\\u00bf\": 91, \"\\\\u2014\": 92, \"\\\\u00e7\": 93, \"#\": 94, \"\\\\u00bd\": 95, \"\\\\u00ed\": 96, \"=\": 97, \"\\\\u2019\": 98, \"\\\\u00e2\": 99, \"\\\\u00a9\": 100, \"\\\\u00bc\": 101, \"+\": 102, \">\": 103, \"$\": 104, \"<\": 105, \"\\\\u00f3\": 106, \"\\\\u00e1\": 107, \"\\\\u00fa\": 108, \"\\\\u00ef\": 109, \"\\\\u00c9\": 110, \"\\\\u00fb\": 111, \"]\": 112, \"[\": 113, \"\\\\u00fc\": 114, \"\\\\u00ea\": 115, \"_\": 116, \"\\\\u00ad\": 117, \"\\\\u00e0\": 118, \"\\\\u00be\": 119, \"\\\\u201a\": 120, \"\\\\ufffd\": 121, \"\\\\u00ba\": 122, \"\\\\u2044\": 123, \"\\\\u00e4\": 124, \"\\\\u00da\": 125, \"\\\\u00f9\": 126, \"\\\\u0301\": 127, \"}\": 128, \"\\\\u00f6\": 129, \"{\": 130, \"\\\\u00ec\": 131, \"\\\\u00f4\": 132, \"\\\\u0096\": 133, \"\\\\u201d\": 134, \"\\\\u00d7\": 135, \"\\\\u02da\": 136, \"\\\\u00bb\": 137, \"@\": 138, \"\\\\u00a7\": 139, \"\\\\\\\\\": 140, \"\\\\u25ca\": 141, \"\\\\u2031\": 142, \"\\\\u201c\": 143, \"\\\\u2027\": 144, \"\\\\u202d\": 145, \"\\\\u215b\": 146, \"\\\\u00e5\": 147, \"\\\\ufb02\": 148, \"`\": 149, \"\\\\u00eb\": 150, \"\\\\u0097\": 151, \"\\\\u001a\": 152, \"\\\\u00f8\": 153, \"\\\\u2153\": 154, \"|\": 155, \"\\\\u01b0\": 156, \"\\\\u0092\": 157, \"\\\\u00b4\": 158, \"\\\\u2012\": 159, \"\\\\u00c2\": 160, \"\\\\u2423\": 161, \"\\\\u00c1\": 162, \"\\\\u00a4\": 163, \"\\\\u201f\": 164, \"\\\\u00a0\": 165, \"\\\\u01a1\": 166, \"\\\\u0103\": 167, \"\\\\u0300\": 168, \"\\\\u215e\": 169, \"\\\\u20ac\": 170, \"~\": 171, \"\\\\u0095\": 172}'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "STOP_SIGN = '␣'\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=True,\n",
        "    filters='',\n",
        "    lower=False,\n",
        "    split=''\n",
        ")\n",
        "\n",
        "# Stop word is not a part of recipes, but tokenizer must know about it as well.\n",
        "tokenizer.fit_on_texts([STOP_SIGN])\n",
        "\n",
        "tokenizer.fit_on_texts(dataset_filtered)\n",
        "\n",
        "tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttmrMRXKXSBe"
      },
      "source": [
        "# Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tVvsU6E_WuIE"
      },
      "outputs": [],
      "source": [
        "VOCABULARY_SIZE = len(tokenizer.word_counts) + 1\n",
        "dataset_vectorized = tokenizer.texts_to_sequences(dataset_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbwCzBJDXpMT"
      },
      "source": [
        "# Sequence to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qWgHVz9_Wvbp"
      },
      "outputs": [],
      "source": [
        "def recipe_sequence_to_string(recipe_sequence):\n",
        "    recipe_stringified = tokenizer.sequences_to_texts([recipe_sequence])[0]\n",
        "    print(recipe_stringified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEel0370YWhR"
      },
      "source": [
        "# Padding sequnces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jv5DVZYFYMRc"
      },
      "outputs": [],
      "source": [
        "dataset_vectorized_padded_without_stops = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    dataset_vectorized,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    # We use -1 here and +1 in the next step to make sure\n",
        "    # that all recipes will have at least 1 stops sign at the end,\n",
        "    # since each sequence will be shifted and truncated afterwards\n",
        "    # (to generate X and Y sequences).\n",
        "    maxlen=MAX_RECIPE_LENGTH-1,\n",
        "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
        ")\n",
        "\n",
        "dataset_vectorized_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    dataset_vectorized_padded_without_stops,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    maxlen=MAX_RECIPE_LENGTH+1,\n",
        "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiXWlKvZhXp"
      },
      "source": [
        "# TF dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QLr__-yNZctA"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(dataset_vectorized_padded)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(dataset_vectorized_padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odqkwN6EZ--Q"
      },
      "source": [
        "# Split into inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_IjBODraFwM"
      },
      "outputs": [],
      "source": [
        "def split_input_target(recipe):\n",
        "    input_text = recipe[:-1]\n",
        "    target_text = recipe[1:]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset_targeted = dataset.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar1IV5sja3vo"
      },
      "source": [
        "# Split dataset into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIy92GWIa3Xe",
        "outputId": "8994b1c3-56d6-4425-92c1-7819925511a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_RepeatDataset element_spec=(TensorSpec(shape=(64, 2000), dtype=tf.int32, name=None), TensorSpec(shape=(64, 2000), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "dataset_train = dataset_targeted.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "\n",
        "print(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vg_RLskb06_"
      },
      "source": [
        "# Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhFTSQDeaoCF",
        "outputId": "1fcdd6ad-4bbd-4c3f-f6db-bbcfe864e637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 251ms/step\n",
            "tmp_input_array shape: (2, 8)\n",
            "tmp_input_array:\n",
            "[[9 2 9 9 8 5 2 4]\n",
            " [4 0 6 1 0 7 4 5]]\n",
            "\n",
            "tmp_output_array shape: (2, 8, 5)\n",
            "tmp_output_array:\n",
            "[[[ 4.5832101e-02 -2.7746785e-02 -1.9866936e-03  2.7656164e-02\n",
            "    9.5329434e-04]\n",
            "  [-3.0400217e-02  2.9302608e-02 -1.5542973e-02 -2.7627409e-02\n",
            "    4.8456166e-02]\n",
            "  [ 4.5832101e-02 -2.7746785e-02 -1.9866936e-03  2.7656164e-02\n",
            "    9.5329434e-04]\n",
            "  [ 4.5832101e-02 -2.7746785e-02 -1.9866936e-03  2.7656164e-02\n",
            "    9.5329434e-04]\n",
            "  [ 4.9754191e-02 -5.5848584e-03  3.2477107e-02 -3.2591835e-02\n",
            "    1.2258410e-02]\n",
            "  [ 4.3745603e-02  4.8818056e-02  1.6985986e-02 -4.2814340e-02\n",
            "    1.3994958e-02]\n",
            "  [-3.0400217e-02  2.9302608e-02 -1.5542973e-02 -2.7627409e-02\n",
            "    4.8456166e-02]\n",
            "  [-8.3633512e-04 -5.3508393e-03  2.1429863e-02 -3.4206916e-02\n",
            "   -5.9641898e-05]]\n",
            "\n",
            " [[-8.3633512e-04 -5.3508393e-03  2.1429863e-02 -3.4206916e-02\n",
            "   -5.9641898e-05]\n",
            "  [ 1.7597202e-02 -3.4577467e-02  3.0482564e-02 -2.5535965e-02\n",
            "    3.4081850e-02]\n",
            "  [-3.5327386e-02  3.1404022e-02 -4.8327923e-02  4.0252749e-02\n",
            "    1.4305238e-02]\n",
            "  [ 1.1549700e-02 -8.1740692e-04  3.4033623e-02  4.8808940e-03\n",
            "   -3.6931731e-02]\n",
            "  [ 1.7597202e-02 -3.4577467e-02  3.0482564e-02 -2.5535965e-02\n",
            "    3.4081850e-02]\n",
            "  [-4.2295277e-02 -1.3649177e-02  2.3229968e-02  2.4187062e-02\n",
            "   -4.9335003e-02]\n",
            "  [-8.3633512e-04 -5.3508393e-03  2.1429863e-02 -3.4206916e-02\n",
            "   -5.9641898e-05]\n",
            "  [ 4.3745603e-02  4.8818056e-02  1.6985986e-02 -4.2814340e-02\n",
            "    1.3994958e-02]]]\n"
          ]
        }
      ],
      "source": [
        "tmp_vocab_size = 10\n",
        "tmp_embedding_size = 5\n",
        "tmp_input_length = 8\n",
        "tmp_batch_size = 2\n",
        "\n",
        "tmp_model = tf.keras.models.Sequential()\n",
        "tmp_model.add(tf.keras.layers.Embedding(\n",
        "  input_dim=tmp_vocab_size,\n",
        "  output_dim=tmp_embedding_size,\n",
        "  input_length=tmp_input_length\n",
        "))\n",
        "# The model will take as input an integer matrix of size (batch, input_length).\n",
        "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
        "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "tmp_input_array = np.random.randint(\n",
        "  low=0,\n",
        "  high=tmp_vocab_size,\n",
        "  size=(tmp_batch_size, tmp_input_length)\n",
        ")\n",
        "tmp_model.compile('rmsprop', 'mse')\n",
        "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIsDLQcmb3-g"
      },
      "source": [
        "#LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vy-5YrjcDPh",
        "outputId": "b4b5be0b-ae71-49d5-cc66-a4eeb3ce8bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           44288     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 173)           177325    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5468589 (20.86 MB)\n",
            "Trainable params: 5468589 (20.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        batch_input_shape=[batch_size, None]\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.LSTM(\n",
        "        units=rnn_units,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size=VOCABULARY_SIZE,\n",
        "  embedding_dim=256,\n",
        "  rnn_units=1024,\n",
        "  batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygwO9B1c9ty"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u72-Z_5EdDEA",
        "outputId": "cfc3abad-6feb-4af6-fbd1-eec6b10068bb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-daa480d1c044>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0madam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model.compile(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# An objective function.\n",
        "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
        "def loss(labels, logits):\n",
        "    entropy = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=labels,\n",
        "      y_pred=logits,\n",
        "      from_logits=True\n",
        "    )\n",
        "\n",
        "    return entropy\n",
        "\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    monitor='loss',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create a checkpoints directory.\n",
        "checkpoint_dir = 'tmp/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "EPOCHS = 500\n",
        "INITIAL_EPOCH = 1\n",
        "STEPS_PER_EPOCH = 1500\n",
        "\n",
        "history = model.fit(\n",
        "    x=dataset_train,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    initial_epoch=INITIAL_EPOCH,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        early_stopping_callback\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Saving the trained model to file (to be able to re-use it later).\n",
        "model_name = 'recipe_generation_rnn_raw.h5'\n",
        "model.save(model_name, save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gEDkwWifVe9"
      },
      "source": [
        "# Rebuild the model for a batch of size 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj5IQWMVfR7p"
      },
      "outputs": [],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "simplified_batch_size = 1\n",
        "\n",
        "model_simplified = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
        "model_simplified.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model_simplified.build(tf.TensorShape([simplified_batch_size, None]))\n",
        "\n",
        "model_simplified.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gicaCn4ngELS"
      },
      "source": [
        "# Prediction loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AWhL2_QgI5k"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    padded_start_string = STOP_WORD_TITLE + start_string\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = np.array(tokenizer.texts_to_sequences([padded_start_string]))\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "            predictions,\n",
        "            num_samples=1\n",
        "        )[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        next_character = tokenizer.sequences_to_texts(input_indices.numpy())[0]\n",
        "\n",
        "        text_generated.append(next_character)\n",
        "\n",
        "    return (padded_start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASjEwZAuhrpE"
      },
      "source": [
        "# Test output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihsrk_vPhrBp"
      },
      "outputs": [],
      "source": [
        "def generate_combinations(model):\n",
        "    recipe_length = 1000\n",
        "    try_letters = ['', '\\n', 'A', 'B', 'C', 'O', 'L', 'Mushroom', 'Apple', 'Slow', 'Christmass', 'The', 'Banana', 'Homemade']\n",
        "    try_temperature = [1.0, 0.8, 0.4, 0.2]\n",
        "\n",
        "    for letter in try_letters:\n",
        "        for temperature in try_temperature:\n",
        "            generated_text = generate_text(\n",
        "                model,\n",
        "                start_string=letter,\n",
        "                num_generate = recipe_length,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            print(f'Attempt: \"{letter}\" + {temperature}')\n",
        "            print('-----------------------------------')\n",
        "            print(generated_text)\n",
        "            print('\\n\\n')\n",
        "\n",
        "generate_combinations(model_simplified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcjnPCzyansK"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgxV7sBj3N8bS0/OLcfZxk",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
