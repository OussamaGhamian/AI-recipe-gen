{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1fTSeOleUnR"
      },
      "outputs": [],
      "source": [
        "# Packages for training the model and working with the dataset.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2DS6VeSfz_h"
      },
      "source": [
        "# Merging and Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFN6FivvemVq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(silent=False):\n",
        "    # List of dataset files we want to merge.\n",
        "    dataset_file_names = [\n",
        "        'recipes_raw_nosource_ar.json',\n",
        "        'recipes_raw_nosource_epi.json',\n",
        "        'recipes_raw_nosource_fn.json',\n",
        "    ]\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for dataset_file_name in dataset_file_names:\n",
        "        dataset_file_path = f'./sample_data/{dataset_file_name}'\n",
        "\n",
        "        with open(dataset_file_path) as dataset_file:\n",
        "            json_data_dict = json.load(dataset_file)\n",
        "            json_data_list = list(json_data_dict.values())\n",
        "            dict_keys = [key for key in json_data_list[0]]\n",
        "            dict_keys.sort()\n",
        "            dataset += json_data_list\n",
        "\n",
        "            # This code block outputs the summary for each dataset.\n",
        "            if silent == False:\n",
        "                print(dataset_file_path)\n",
        "                print('===========================================')\n",
        "                print('Number of examples: ', len(json_data_list), '\\n')\n",
        "                print('Example object keys:\\n', dict_keys, '\\n')\n",
        "                print('Example object:\\n', json_data_list[0], '\\n')\n",
        "                print('Required keys:\\n')\n",
        "                print('  title: ', json_data_list[0]['title'], '\\n')\n",
        "                print('  ingredients: ', json_data_list[0]['ingredients'], '\\n')\n",
        "                print('  instructions: ', json_data_list[0]['instructions'])\n",
        "                print('\\n\\n')\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset_raw = load_dataset(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChWWG2FLgekS"
      },
      "source": [
        "# Filter out recipe with missing feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR7CbB4IexsX"
      },
      "outputs": [],
      "source": [
        "def recipe_validate_required_fields(recipe):\n",
        "    required_keys = ['title', 'ingredients', 'instructions']\n",
        "\n",
        "    if not recipe:\n",
        "        return False\n",
        "\n",
        "    for required_key in required_keys:\n",
        "        if not recipe[required_key]:\n",
        "            return False\n",
        "\n",
        "        if type(recipe[required_key]) == list and len(recipe[required_key]) == 0:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "dataset_validated = [recipe for recipe in dataset_raw if recipe_validate_required_fields(recipe)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKQVeTNBUNug"
      },
      "source": [
        "# Stringify recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D_Ocx4VgnxC"
      },
      "outputs": [],
      "source": [
        "STOP_WORD_TITLE = '???? '\n",
        "STOP_WORD_INGREDIENTS = '\\n????\\n\\n'\n",
        "STOP_WORD_INSTRUCTIONS = '\\n????\\n\\n'\n",
        "\n",
        "def recipe_to_string(recipe):\n",
        "    # This string is presented as a part of recipes so we need to clean it up.\n",
        "    noize_string = 'ADVERTISEMENT'\n",
        "\n",
        "    title = recipe['title']\n",
        "    ingredients = recipe['ingredients']\n",
        "    instructions = recipe['instructions'].split('\\n')\n",
        "\n",
        "    ingredients_string = ''\n",
        "    for ingredient in ingredients:\n",
        "        ingredient = ingredient.replace(noize_string, '')\n",
        "        if ingredient:\n",
        "            ingredients_string += f'• {ingredient}\\n'\n",
        "\n",
        "    instructions_string = ''\n",
        "    for instruction in instructions:\n",
        "        instruction = instruction.replace(noize_string, '')\n",
        "        if instruction:\n",
        "            instructions_string += f'▪︎ {instruction}\\n'\n",
        "\n",
        "    return f'{STOP_WORD_TITLE}{title}\\n{STOP_WORD_INGREDIENTS}{ingredients_string}{STOP_WORD_INSTRUCTIONS}{instructions_string}'\n",
        "\n",
        "dataset_stringified = [recipe_to_string(recipe) for recipe in dataset_validated]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4pbryUVrpg"
      },
      "source": [
        "# Filter out long recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6gveOvIVPMZ"
      },
      "outputs": [],
      "source": [
        "MAX_RECIPE_LENGTH = 2000\n",
        "def filter_recipes_by_length(recipe_test):\n",
        "    return len(recipe_test) <= MAX_RECIPE_LENGTH\n",
        "\n",
        "dataset_filtered = [recipe_text for recipe_text in dataset_stringified if filter_recipes_by_length(recipe_text)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qfJJkEsWW9x"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QbKylCFVyRl",
        "outputId": "97f0b222-39a4-4581-983d-036e12449a7d"
      },
      "outputs": [],
      "source": [
        "STOP_SIGN = '␣'\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=True,\n",
        "    filters='',\n",
        "    lower=False,\n",
        "    split=''\n",
        ")\n",
        "\n",
        "# Stop word is not a part of recipes, but tokenizer must know about it as well.\n",
        "tokenizer.fit_on_texts([STOP_SIGN])\n",
        "\n",
        "tokenizer.fit_on_texts(dataset_filtered)\n",
        "\n",
        "tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttmrMRXKXSBe"
      },
      "source": [
        "# Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVvsU6E_WuIE"
      },
      "outputs": [],
      "source": [
        "VOCABULARY_SIZE = len(tokenizer.word_counts) + 1\n",
        "dataset_vectorized = tokenizer.texts_to_sequences(dataset_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbwCzBJDXpMT"
      },
      "source": [
        "# Sequence to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWgHVz9_Wvbp"
      },
      "outputs": [],
      "source": [
        "def recipe_sequence_to_string(recipe_sequence):\n",
        "    recipe_stringified = tokenizer.sequences_to_texts([recipe_sequence])[0]\n",
        "    print(recipe_stringified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEel0370YWhR"
      },
      "source": [
        "# Padding sequnces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv5DVZYFYMRc"
      },
      "outputs": [],
      "source": [
        "dataset_vectorized_padded_without_stops = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    dataset_vectorized,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    # We use -1 here and +1 in the next step to make sure\n",
        "    # that all recipes will have at least 1 stops sign at the end,\n",
        "    # since each sequence will be shifted and truncated afterwards\n",
        "    # (to generate X and Y sequences).\n",
        "    maxlen=MAX_RECIPE_LENGTH-1,\n",
        "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
        ")\n",
        "\n",
        "dataset_vectorized_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    dataset_vectorized_padded_without_stops,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    maxlen=MAX_RECIPE_LENGTH+1,\n",
        "    value=tokenizer.texts_to_sequences([STOP_SIGN])[0]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiXWlKvZhXp"
      },
      "source": [
        "# TF dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLr__-yNZctA"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(dataset_vectorized_padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odqkwN6EZ--Q"
      },
      "source": [
        "# Split into inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_IjBODraFwM"
      },
      "outputs": [],
      "source": [
        "def split_input_target(recipe):\n",
        "    input_text = recipe[:-1]\n",
        "    target_text = recipe[1:]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset_targeted = dataset.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar1IV5sja3vo"
      },
      "source": [
        "# Split dataset into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIy92GWIa3Xe",
        "outputId": "8994b1c3-56d6-4425-92c1-7819925511a9"
      },
      "outputs": [],
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "dataset_train = dataset_targeted.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "\n",
        "print(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vg_RLskb06_"
      },
      "source": [
        "# Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhFTSQDeaoCF",
        "outputId": "1fcdd6ad-4bbd-4c3f-f6db-bbcfe864e637"
      },
      "outputs": [],
      "source": [
        "tmp_vocab_size = 10\n",
        "tmp_embedding_size = 5\n",
        "tmp_input_length = 8\n",
        "tmp_batch_size = 2\n",
        "\n",
        "tmp_model = tf.keras.models.Sequential()\n",
        "tmp_model.add(tf.keras.layers.Embedding(\n",
        "  input_dim=tmp_vocab_size,\n",
        "  output_dim=tmp_embedding_size,\n",
        "  # input_length=tmp_input_length\n",
        "))\n",
        "# The model will take as input an integer matrix of size (batch, input_length).\n",
        "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
        "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "tmp_input_array = np.random.randint(\n",
        "  low=0,\n",
        "  high=tmp_vocab_size,\n",
        "  size=(tmp_batch_size, tmp_input_length)\n",
        ")\n",
        "tmp_model.compile('rmsprop', 'mse')\n",
        "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIsDLQcmb3-g"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vy-5YrjcDPh",
        "outputId": "b4b5be0b-ae71-49d5-cc66-a4eeb3ce8bf0"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, is_lstm=True):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim\n",
        "    ))\n",
        "    if(is_lstm):\n",
        "        model.add(tf.keras.layers.LSTM(\n",
        "            units=rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "        ))\n",
        "    else:\n",
        "        model.add(\n",
        "            tf.keras.layers.GRU(units=rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "            ))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size=VOCABULARY_SIZE,\n",
        "  embedding_dim=256,\n",
        "  rnn_units=1024,\n",
        "  batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygwO9B1c9ty"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u72-Z_5EdDEA",
        "outputId": "cfc3abad-6feb-4af6-fbd1-eec6b10068bb"
      },
      "outputs": [],
      "source": [
        "# An objective function.\n",
        "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
        "def loss(labels, logits):\n",
        "    entropy = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=labels,\n",
        "      y_pred=logits,\n",
        "      from_logits=True\n",
        "    )\n",
        "\n",
        "    return entropy\n",
        "\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    monitor='loss',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create a checkpoints directory.\n",
        "checkpoint_dir = 'tmp/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "EPOCHS = 500\n",
        "INITIAL_EPOCH = 1\n",
        "STEPS_PER_EPOCH = 1500\n",
        "\n",
        "history = model.fit(\n",
        "    x=dataset_train,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    initial_epoch=INITIAL_EPOCH,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        early_stopping_callback\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Saving the trained model to file (to be able to re-use it later).\n",
        "model_name = 'recipe_generation_rnn_raw.h5'\n",
        "model.save(model_name, save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gEDkwWifVe9"
      },
      "source": [
        "# Rebuild the model for a batch of size 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj5IQWMVfR7p"
      },
      "outputs": [],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "simplified_batch_size = 1\n",
        "\n",
        "model_simplified = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
        "model_simplified.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model_simplified.build(tf.TensorShape([simplified_batch_size, None]))\n",
        "\n",
        "model_simplified.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gicaCn4ngELS"
      },
      "source": [
        "# Prediction loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AWhL2_QgI5k"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    padded_start_string = STOP_WORD_TITLE + start_string\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = np.array(tokenizer.texts_to_sequences([padded_start_string]))\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "            predictions,\n",
        "            num_samples=1\n",
        "        )[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        next_character = tokenizer.sequences_to_texts(input_indices.numpy())[0]\n",
        "\n",
        "        text_generated.append(next_character)\n",
        "\n",
        "    return (padded_start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASjEwZAuhrpE"
      },
      "source": [
        "# Test output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihsrk_vPhrBp"
      },
      "outputs": [],
      "source": [
        "def generate_combinations(model):\n",
        "    recipe_length = 1000\n",
        "    try_letters = ['', '\\n', 'A', 'B', 'C', 'O', 'L', 'Mushroom', 'Apple', 'Slow', 'Christmass', 'The', 'Banana', 'Homemade']\n",
        "    try_temperature = [1.0, 0.8, 0.4, 0.2]\n",
        "\n",
        "    for letter in try_letters:\n",
        "        for temperature in try_temperature:\n",
        "            generated_text = generate_text(\n",
        "                model,\n",
        "                start_string=letter,\n",
        "                num_generate = recipe_length,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            print(f'Attempt: \"{letter}\" + {temperature}')\n",
        "            print('-----------------------------------')\n",
        "            print(generated_text)\n",
        "            print('\\n\\n')\n",
        "\n",
        "generate_combinations(model_simplified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcjnPCzyansK"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgxV7sBj3N8bS0/OLcfZxk",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
